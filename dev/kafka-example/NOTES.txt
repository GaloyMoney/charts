# deploying the kafka-example components

# use this to build a new image - it builds and runs the kafka-conncet pod afterwards
tf apply -target kubernetes_manifest.kafka_connect_build --auto-approve

  k describe KafkaConnect -A
  k -n galoy-dev-kafka   logs     kafka-connect-connect-build
  tf destroy -target kubernetes_manifest.kafka_connect_build --auto-approve
  k -n galoy-dev-kafka  delete pod      kafka-connect-connect-build --force
# use this when there is an image available already
tf apply -target kubernetes_manifest.kafka_connect --auto-approve

tf apply -target kubernetes_manifest.kafka_topic --auto-approve
k describe KafkaTopic -A

tf apply -target kubernetes_manifest.kafka_source_connector --auto-approve

tf apply -target kubernetes_manifest.kafka_sink_connector --auto-approve


# manually create events in the kafka pod
bin/kafka-console-producer.sh --topic my-topic --bootstrap-server localhost:9093

# check what data has kafka received
kubectl -n galoy-dev-kafka exec kafka-kafka-0 -i -t -- bin/kafka-console-consumer.sh --bootstrap-server kafka-kafka-bootstrap.galoy-dev-kafka.svc.cluster.local:9093 --topic my-topic --from-beginning

# list the KafkaConnector kinds (source and sink)
kubectl get KafkaConnector --selector strimzi.io/cluster=kafka-connect -o name

## Build docs
https://github.com/strimzi/strimzi-kafka-operator/blob/0.33.2/examples/connect/kafka-connect-build.yaml
https://strimzi.io/blog/2021/03/29/connector-build/
https://github.com/strimzi/strimzi-kafka-operator/blob/main/documentation/api/io.strimzi.api.kafka.model.connect.build.Build.adoc
## Create Docker credentials:
#https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
 docker login
 cat ~/.docker/config.json
 kubectl -n galoy-dev-kafka create secret generic regcred \
    --from-file=.dockerconfigjson=$HOME/.docker/config.json \
    --type=kubernetes.io/dockerconfigjson
 kubectl -n galoy-dev-kafka get secret regcred --output=yaml

# TODO
* create a permanent docker image store - can be personal docker hub for testing
* find a reliable way to test the data flow
* introduce the mongodb source connector
* introduce the bigquery sink connector



## Current blocker:

k -n galoy-dev-kafka logs -f $(kubectl -n galoy-dev-kafka get pods | grep connect | awk '{print $1}')


WARN [kafka-sink-connector|task-1] [Consumer clientId=connector-consumer-kafka-sink-connector-1, groupId=connect-kafka-sink-connector] Error while fetching metadata with correlation id 1986 : {my-topic=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient) [task-thread-kafka-sink-connector-1]
WARN [kafka-source-connector|task-0] [Producer clientId=connector-producer-kafka-source-connector-0] Error while fetching metadata with correlation id 1920 : {my-topic=UNKNOWN_TOPIC_OR_PARTITION} (org.apache.kafka.clients.NetworkClient) [kafka-producer-network-thread | connector-producer-kafka-source-connector-0]

